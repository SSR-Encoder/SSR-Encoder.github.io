<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>SSR-Encoder</title>
<link href="./SSR-Encoder_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./SSR-Encoder_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./SSR-Encoder_files/jquery.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>

<body>
<div class="content">
  <h1><strong>SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation</strong></h1>
  <p id="authors">
  <span>Yuxuan Zhang,</span>
  <span>Jiaming Liu,</span>
  <span>Yiren Song,</span>
  <span>Rui Wang,</span>
  <span>Hao Tang,</span> 
  <span>Jinpeng Yu,</span> <br>
  <span>Huaxia Li,</span>
  <span>Xu Tang,</span>
  <span>Yao Hu,</span>
  <span>Han Pan,</span>
  <span>Zhongliang Jing</span>
</p> <br>
<div style="text-align: center;">
  <span style="font-size: 24px">Shanghai Jiao Tong University, Xiaohongshu Inc., Beijing University Of Posts And Telecommunications, Carnegie Mellon University, ShanghaiTech University
  </span></p>
</div>
  <br>
  <img src="./SSR-Encoder_files/teaser_v3.png" class="teaser-gif" style="width:100%;"><br>
  <h3 style="text-align:center; font-size: 16px;"><em>Our SSR-Encoder is a model generalizable encoder, which is able to guide any customized diffusion models for single subject-driven image generation (top branch) or multiple subject-driven image generation from different images (middle branch) based on the image representation selected by the text query or mask query without any additional test-time finetuning. Furthermore, our SSR-Encoder can also be applied for the controllable generation with additional control (bottom branch).</em></h3>
    <font size="+2">
          <p style="text-align: center;">
            <a href="ssr-encoder.github.io" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Recent advancements in subject-driven image generation have led to zero-shot generation, yet precise selection and focus on crucial subject representations remain challenging. Addressing this, we introduce the SSR-Encoder, a novel architecture designed for selectively capturing any subject from single or multiple reference images. It responds to various query modalities including text and masks, without necessitating test-time fine-tuning. The SSR-Encoder combines a Token-to-Patch Aligner that aligns query inputs with image patches and a Detail-Preserving Subject Encoder for extracting and preserving fine features of the subjects, thereby generating subject embeddings. These embeddings, used in conjunction with original text embeddings, condition the generation process. Characterized by its model generalizability and efficiency, the SSR-Encoder adapts to a range of custom models and control modules. Enhanced by the Embedding Consistency Regularization Loss for improved training, our extensive experiments demonstrate its effectiveness in versatile and high-quality image generation, indicating its broad applicability.</p>
</div>
<div class="content">
  <h2>Background</h2>
  <p>Recent advancements in image generation, especially with the advent of text-to-image diffusion models trained on extensive datasets, have revolutionized this field. A prime example is Stable Diffusion, an open-source model, which allows a broad user base to easily generate images from textual prompts. A growing area of interest that has emerged is the subject-driven generation, where the focus shifts from creating a generic subject, like ``a cat'' to generating a specific instance, such as ``the cat''. However, crafting the perfect text prompt to generate the desired subject content poses a significant challenge. Consequently, researchers are exploring various strategies for effective subject-driven generation. Subject-driven image generation aims to learn subjects from reference images and generate images aligning with specific concepts like identity and style. Currently, one prominent approach involves test-time fine-tuning, which, while efficient, requires substantial computational resources to learn each new subject. Another approach encodes the reference image into an image embedding to bypass the fine-tuning cost. However, these encoder-based models typically require joint training with the base diffusion model, limiting their generalizability. A concurrent work, IP-adapter, tackles both fine-tuning costs and generalizability by learning a projection to inject image information into the U-Net, avoiding the need to fine-tune the base text-to-image model, thereby broadening its application in personalized models. Despite these advancements, a critical aspect often overlooked is the extraction of the most informative representation of a subject. With images being a complex mixture of subjects, backgrounds, and styles, it's vital to focus on the most crucial elements to represent a subject effectively. To address this, we introduce the SSR-Encoder, an image encoder that generates Selective Subject Representations for subject-driven image generation. </p>
  <br>
</div>
<div class="content">
  <h2>Approach</h2>
<p>Selective subject-driven image generation aims to generate target subjects in a reference image with high fidelity and creative editability, guided by the user's specific queries (text or mask). To tackle this, we propose our SSR-Encoder, a specialized framework designed to integrate with any custom diffusion model without necessitating test-time fine-tuning. Formally, for a given reference image <span class="tex">\mathit{I}</span> and a user query <span class="tex">\mathit{q}</span>, the SSR-Encoder effectively captures subject-specific information and generates multi-scale subject embeddings <span class="tex">\mathit{c_s}</span>. These multi-scale subject embeddings <span class="tex">\mathit{c_s}</span> are subsequently integrated into the U-Net model with trainable copies of cross-attention layers. The generation process, conditioned on both subject embeddings <span class="tex">c_s</span> and text embedding <span class="tex">c_t</span>, allows for the production of desired subjects with high fidelity and creative editability.</p>
  <br>
  <img class="summary-img" src="./SSR-Encoder_files/method_v7.png" style="width:100%;"> <br>
  <p>Given a query text-image pairs <span class="tex">$$\left(q, I\right)$$</span>, the SSR-Encoder employs a token-to-patch aligner to highlight the selective regions in the reference image by the query. It extracts more fine-grained details of the subject through the detail-preserving subject encoder, projecting multi-scale visual embeddings via the token-to-patch aligner. Then, we adopt subject-conditioned generation to generate specific subjects with high fidelity and creative editability. During training, we adopt reconstruction loss <span class="tex">L_{LDM}</span> and embedding consistency regularization loss <span class="tex">L_{reg}</span> for selective subject-driven learning.</p>
  <br>
</div>
<div class="content">
  <h2>Results</h2>
  <p>Results of SSR-Encoder in different generative capabilities. Our method supports two query modalities and is adaptable for a variety of tasks, including single- and multi-subject conditioned generation. Its versatility extends to integration with other customized models and compatibility with off-the-shelf ControlNets. </p>
<img class="summary-img" src="./SSR-Encoder_files/images_01_7.png" style="width:100%;">
</div>
<div class="content">
  <h2>Attention visualization</h2>
  <p>Visualization of attention maps <span class="tex">A_{t2p}</span> in SSR-Encoder.</p>
  <br>
  <img class="summary-img" src="./SSR-Encoder_files/images_07.jpg" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Selective subject-driven image generation</h2>
  <p>Select different subject from a single image for re-contextualization.</p>
  <br>
  <img class="summary-img" src="./SSR-Encoder_files/sup_ms1.jpg" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Human image generation</h2>
  <p>Despite the SSR-Encoder not being trained in domain-specific settings (such as human faces), it is already capable of capturing the intricate details of the subjects. We utilize face images from the OpenImages dataset as reference images for generating human images. To better illustrate our results, we also employ images of two celebrities as references.</p>  
  <br>
  <img class="summary-img" src="./SSR-Encoder_files/sup_face.jpg" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Generalizable to video generation models</h2>
  <p>Our SSR-Encoder is not only versatile enough to adapt to various custom models and controllability modules, but it can also be effectively applied to video generation, integrating seamlessly with video generation models. We demonstrate the impact of combining our SSR-Encoder with Animatediff. Despite not being trained on video data, our method can flawlessly combine with Animatediff to produce videos that maintain consistent character identities with reference images.</p>
  <br>
  <img class="summary-img" src="./SSR-Encoder_files/sup_animate.jpg" style="width:100%;"> <br>
</div>
</body>
</html>
