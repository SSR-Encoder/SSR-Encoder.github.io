<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>SSR-Encoder</title>
<link href="./SSR-Encoder_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./SSR-Encoder_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./SSR-Encoder_files/jquery.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$', '$']]},
      messageStyle: "none"
  });
</script>
</head>

<body>
<div class="content">
  <h1><strong>SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation</strong></h1>
  <p id="authors">
  <span>Yuxuan Zhang,</span>
  <span>Jiaming Liu,</span>
  <span>Yiren Song,</span>
  <span>Rui Wang,</span>
  <span>Hao Tang,</span> 
  <span>Jinpeng Yu,</span> <br>
  <span>Huaxia Li,</span>
  <span>Xu Tang,</span>
  <span>Yao Hu,</span>
  <span>Han Pan,</span>
  <span>Zhongliang Jing</span>
</p> <br>
<div style="text-align: center;">
  <span style="font-size: 24px">Shanghai Jiao Tong University, Xiaohongshu Inc., Beijing University Of Posts And Telecommunications, Carnegie Mellon University, ShanghaiTech University
  </span></p>
</div>
  <br>
  <img src="./SSR-Encoder_files/teaser_v3.png" class="teaser-gif" style="width:100%;"><br>
  <h3 style="text-align:center; font-size: 16px;"><em>Our SSR-Encoder is a model generalizable encoder, which is able to guide any customized diffusion models for single subject-driven image generation (top branch) or multiple subject-driven image generation from different images (middle branch) based on the image representation selected by the text query or mask query without any additional test-time finetuning. Furthermore, our SSR-Encoder can also be applied for the controllable generation with additional control (bottom branch).</em></h3>
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://arxiv.org/abs/2208.12242" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Recent advancements in subject-driven image generation have led to zero-shot generation, yet precise selection and focus on crucial subject representations remain challenging. Addressing this, we introduce the SSR-Encoder, a novel architecture designed for selectively capturing any subject from single or multiple reference images. It responds to various query modalities including text and masks, without necessitating test-time fine-tuning. The SSR-Encoder combines a Token-to-Patch Aligner that aligns query inputs with image patches and a Detail-Preserving Subject Encoder for extracting and preserving fine features of the subjects, thereby generating subject embeddings. These embeddings, used in conjunction with original text embeddings, condition the generation process. Characterized by its model generalizability and efficiency, the SSR-Encoder adapts to a range of custom models and control modules. Enhanced by the Embedding Consistency Regularization Loss for improved training, our extensive experiments demonstrate its effectiveness in versatile and high-quality image generation, indicating its broad applicability.</p>
</div>
<div class="content">
  <h2>Background</h2>
  <p>Recent advancements in image generation, especially with the advent of text-to-image diffusion models trained on extensive datasets, have revolutionized this field. A prime example is Stable Diffusion, an open-source model, which allows a broad user base to easily generate images from textual prompts. A growing area of interest that has emerged is the subject-driven generation, where the focus shifts from creating a generic subject, like ``a cat'' to generating a specific instance, such as ``the cat''. However, crafting the perfect text prompt to generate the desired subject content poses a significant challenge. Consequently, researchers are exploring various strategies for effective subject-driven generation. Subject-driven image generation aims to learn subjects from reference images and generate images aligning with specific concepts like identity and style. Currently, one prominent approach involves test-time fine-tuning, which, while efficient, requires substantial computational resources to learn each new subject. Another approach encodes the reference image into an image embedding to bypass the fine-tuning cost. However, these encoder-based models typically require joint training with the base diffusion model, limiting their generalizability. A concurrent work, IP-adapter, tackles both fine-tuning costs and generalizability by learning a projection to inject image information into the U-Net, avoiding the need to fine-tune the base text-to-image model, thereby broadening its application in personalized models. Despite these advancements, a critical aspect often overlooked is the extraction of the most informative representation of a subject. With images being a complex mixture of subjects, backgrounds, and styles, it's vital to focus on the most crucial elements to represent a subject effectively. To address this, we introduce the SSR-Encoder, an image encoder that generates Selective Subject Representations for subject-driven image generation. </p>
  <br>
</div>
<div class="content">
  <h2>Approach</h2>
<p>Selective subject-driven image generation aims to generate target subjects in a reference image with high fidelity and creative editability, guided by the user's specific queries (text or mask). To tackle this, we propose our SSR-Encoder, a specialized framework designed to integrate with any custom diffusion model without necessitating test-time fine-tuning. Formally, for a given reference image <span class="tex">\mathit{I}</span> and a user query <span class="tex">\mathit{q}</span>, the SSR-Encoder effectively captures subject-specific information and generates multi-scale subject embeddings <span class="tex">\mathit{c_s}</span>. These multi-scale subject embeddings <span class="tex">\mathit{c_s}</span> are subsequently integrated into the U-Net model with trainable copies of cross-attention layers. The generation process, conditioned on both subject embeddings <span class="tex">c_s</span> and text embedding <span class="tex">c_t</span>, allows for the production of desired subjects with high fidelity and creative editability.</p>
  <br>
  <img class="summary-img" src="./SSR-Encoder_files/method_v7.png" style="width:100%;"> <br>
  <p>Given a query text-image pairs <span class="tex">$\left(q, I\right)$</span>, the SSR-Encoder employs a token-to-patch aligner to highlight the selective regions in the reference image by the query. It extracts more fine-grained details of the subject through the detail-preserving subject encoder, projecting multi-scale visual embeddings via the token-to-patch aligner. Then, we adopt subject-conditioned generation to generate specific subjects with high fidelity and creative editability. During training, we adopt reconstruction loss <span class="tex">L_{LDM}</span> and embedding consistency regularization loss <span class="tex">L_{reg}</span> for selective subject-driven learning.</p>
  <br>
</div>
<div class="content">
  <h2>Results</h2>
  <p>Results of SSR-Encoder in different generative capabilities. Our method supports two query modalities and is adaptable for a variety of tasks, including single- and multi-subject conditioned generation. Its versatility extends to integration with other customized models and compatibility with off-the-shelf ControlNets. </p>
<img class="summary-img" src="./SSR-Encoder_files/images_01_7.png" style="width:100%;">
</div>
<div class="content">
  <h2>Attention visualization</h2>
  <p>Visualization of attention maps <math>A_{t2p}</math>.</p>
  <br>
  <img class="summary-img" src="./SSR-Encoder_files/images_07.jpg" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Text-Guided View Synthesis</h2>
  <p>Our technique can synthesized images with specified viewpoints for a subject cat (left to right: top, bottom, side and back views). Note that the generated poses are  different from the input poses, and the background changes in a realistic manner given a pose change. We also highlight the preservation of complex fur patterns on the subject cat's forehead.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/novel_views.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Property Modification</h2>
  <p>We show color modifications in the first row (using prompts ``a [color] [V] car''), and crosses between a specific dog and different animals in the second row (using prompts ``a cross of a [V] dog and a [target species]''). We highlight the fact that our method preserves unique visual features that give the subject its identity or essence, while performing the required property modification.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/property_modification.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Accessorization</h2>
  <p>Outfitting a dog with accessories. The identity of the subject is preserved and many different outfits or accessories can be applied to the dog given a prompt of type <em>"a [V] dog wearing a police/chef/witch outfit''</em>. We observe a realistic interaction between the subject dog and the outfits or accessories, as well as a large variety of possible options.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/accessories.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Societal Impact</h2>
  <p>This project aims to provide users with an effective tool for synthesizing personal subjects (animals, objects) in different contexts. While general text-to-image models might be biased towards specific attributes when synthesizing images from text, our approach enables the user to get a better reconstruction of their desirable subjects. On contrary, malicious parties might try to use such images to mislead viewers. This is a common issue, existing in other generative models approaches or content manipulation techniques. Future research in generative modeling, and specifically of personalized generative priors, must continue investigating and revalidating these concerns.</p>
  <br>
</div>
<div class="content">
  <h2>BibTex</h2>
  <code> @article{ruiz2022dreambooth,<br>
  &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},<br>
  &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code> 
</div>
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or for their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained models of Imagen. Finally, a special thank you to David Salesin for his feedback, advice and for his support for the project.
    <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). --> 
  </p>
</div>
</body>
</html>
